{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" (Gaussian) Naive Bayes\n",
    "P(A|B) = P(B|A)*P(A)/P(B)\n",
    "\n",
    "Assumes the probability of each attribute belonging to a given class \n",
    "value is independent of all other attributes. We will model p(data|class)\n",
    "as p(x_i|y) = 1/np.sqrt(2*np.pi*np.std(x in class y)) * np.exp(-(x-np.mean(x in class y))/(2*np.var(x in class y))).\n",
    "\n",
    "IMPROVEMENT IDEAS: smoothing (what happens when a class never see a feature?), how would we do this for strings,\n",
    "log the probabilities so they suffer less from numerical roundoff, multinomial naive bayes, how to make the independence\n",
    "assumption less strong (e.g. bigrams)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = pd.read_csv('../data/pima.txt', header=None)\n",
    "\n",
    "# Extract labels assuming last col is our labels column\n",
    "labels = sorted(data.iloc[:, -1].unique())\n",
    "\n",
    "# Split into classes based on labels and extract predictor features\n",
    "# NOTE: these are computed in order of sorted labels. We assume this\n",
    "# in the predict fnc defined below for how we output a value.\n",
    "class_features = [data[data.iloc[:, -1] == i] for i in labels]\n",
    "class_features = [c.iloc[:, :-1] for c in class_features]\n",
    "\n",
    "# Compute means and variances for each class\n",
    "class_stats = [(c.mean(), c.var())  for c in class_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_x_given_y(x, y_stats):\n",
    "    \"\"\" Compute Gaussian probability density function given a feature vector x,\n",
    "    the mean of feature vector x in class y, and the variance of features in \n",
    "    class y.\n",
    "    \"\"\"\n",
    "    y_mean, y_var = y_stats[0], y_stats[1]\n",
    "    density = 1/(np.sqrt(2*np.pi*y_var)) * np.exp(-(((x-y_mean)**2)/(2*y_var)))\n",
    "    return np.prod(density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, class_stats, labels):\n",
    "    \"\"\" Predict probabilities for each class using Gaussian Naive Bayes,\n",
    "    return its label\n",
    "    \"\"\"\n",
    "    probabilities =  [p_x_given_y(x, y_stats) for y_stats in class_stats]\n",
    "    return labels[np.argmax(probabilities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy across dataset: 0.75\n"
     ]
    }
   ],
   "source": [
    "correct, total = 0, len(data)\n",
    "for i in range(total):\n",
    "    \n",
    "    # Extract out features and label\n",
    "    x, y = data.iloc[i, :-1], data.iloc[i, -1]\n",
    "    \n",
    "    # Argmax over probabilities for each class\n",
    "    pred = predict(x, class_stats, labels)\n",
    "    \n",
    "    # Keep track of accuracy\n",
    "    if pred == y:\n",
    "        correct += 1\n",
    "        \n",
    "print('Accuracy across dataset: %.2f'%(correct/total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
